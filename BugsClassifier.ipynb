{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides possibility to:\n",
    " - download bugs data from mozilla\n",
    " - perform data munging on downloaded data and create datasets with different parameters\n",
    " - create, train neural network model to classify bugs by product - component labels\n",
    " - test neural networks on separated test data.\n",
    " - automatically search best hyperparameters for neural network models.\n",
    " - save information about models to history file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import io\n",
    "import gc\n",
    "import re\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import requests\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import reduce\n",
    "import json\n",
    "import collections\n",
    "from time import strftime\n",
    "import sklearn\n",
    "\n",
    "#import concurrent.futures\n",
    "#from requests_futures.sessions import FuturesSession\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Dense, Input, Flatten, Average, SpatialDropout1D, LeakyReLU\n",
    "from keras.layers import AveragePooling1D, GlobalAveragePooling1D, GlobalMaxPool1D, MaxPooling1D\n",
    "from keras.layers import Conv1D, Embedding, Merge, Dropout, Activation\n",
    "from keras.layers import BatchNormalization, Concatenate\n",
    "import keras.callbacks\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import sklearn.metrics\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.word2vec import Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First step - define global directories constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_C = os.path.join(os.getcwd(), 'data')\n",
    "MODELS_DIR_C = os.path.join(os.getcwd(), 'models')\n",
    "LOGS_DIR_C = os.path.join(os.getcwd(), 'logs')\n",
    "if not os.path.exists(DATA_DIR_C):\n",
    "    os.makedirs(DATA_DIR_C)\n",
    "if not os.path.exists(MODELS_DIR_C):\n",
    "    os.makedirs(MODELS_DIR_C)\n",
    "if not os.path.exists(LOGS_DIR_C):\n",
    "    os.makedirs(LOGS_DIR_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next step - define functions for downloading data from bugzilla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://bugzilla.mozilla.org/buglist.cgi?query_format=advanced&resolution=FIXED&resolution=INVALID&ctype=csv\n",
    "def downloadExtendedData(data_frame):\n",
    "    commentUrls = ['https://bugzilla.mozilla.org' + '/rest/bug/' + str(bugId) + '/comment' for bugId in data_frame.bug_id]\n",
    "\n",
    "    async def downloadUrls(sUrls, data_frame):\n",
    "       # with FuturesSession(max_workers=20) as session:\n",
    "        remainingUrls = sUrls\n",
    "        with ThreadPoolExecutor(max_workers=100) as executor:\n",
    "            while len(remainingUrls) > 0:\n",
    "                loop = asyncio.get_event_loop()\n",
    "                futures = [loop.run_in_executor(executor, requests.get, sUrl) for sUrl in remainingUrls]\n",
    "                print('futures count = ', len(futures))\n",
    "                rCount = 0\n",
    "                remainingUrlsIndices = []\n",
    "                for response in await asyncio.gather(*futures, return_exceptions=True):\n",
    "                    if isinstance(response, Exception):\n",
    "                        remainingUrlsIndices.append(rCount)\n",
    "                    else:\n",
    "                        if len(response.text) > 0:\n",
    "                            try:\n",
    "                                jData = response.json()\n",
    "                                for sId, obj in jData['bugs'].items():\n",
    "                                    data_frame.set_value(int(sId), 'description', obj['comments'][0]['text'])\n",
    "                            except Exception as exc:\n",
    "                                print('incorrect json response or problem with data_frame. Response = ', response)\n",
    "                    rCount += 1\n",
    "                print('gathered count = ', rCount - len(remainingUrlsIndices))\n",
    "                if len(remainingUrlsIndices) > 0:\n",
    "                    gc.collect()\n",
    "                    time.sleep(4)\n",
    "                    remainingUrls = [remainingUrls[i] for i in remainingUrlsIndices]\n",
    "                else:\n",
    "                    remainingUrls = []\n",
    "                \n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(downloadUrls(commentUrls, data_frame))\n",
    "    print('downloadExtendedData finished')\n",
    "\n",
    "# provides possibility of saving data after exceptions\n",
    "dataAll = pd.DataFrame()\n",
    "\n",
    "def downloadData(resolutions = ['FIXED'], \n",
    "                 sDir = DATA_DIR_C, sName = 'bugData.csv', nMax = 100000,                 \n",
    "                 columns = [\"bug_id\", \"opendate\", \"cc_count\", \"keywords\", \"longdescs.count\", \"priority\",\n",
    "                            \"classification\", \"product\", \"component\", \"bug_status\", \"resolution\", \"short_desc\",\n",
    "                            \"rep_platform\", \"op_sys\", \"reporter\", \"version\"],\n",
    "                 components = {}):\n",
    "    global dataAll\n",
    "    resList = [\"---\", \"FIXED\", \"INVALID\", \"WONTFIX\", \"DUPLICATE\", \"WORKSFORME\", \"INCOMPLETE\",\n",
    "               \"SUPPORT\", \"EXPIRED\", \"MOVED\"]\n",
    "    columnsList = [\"bug_id\", \"opendate\", \"cc_count\", \"keywords\", \"longdescs.count\", \"priority\", \"classification\", \n",
    "                   \"product\", \"component\", \"bug_status\", \"resolution\", \"short_desc\",\n",
    "                   \"rep_platform\", \"op_sys\", \"reporter\", \"version\"]\n",
    "    if not os.path.isdir(sDir):\n",
    "        return False\n",
    "    baseUrl = 'https://bugzilla.mozilla.org/buglist.cgi?query_format=advanced&columnlist='\n",
    "    \n",
    "    bValid = False\n",
    "    for colName in columns:\n",
    "        if colName in columnsList:\n",
    "            if bValid:\n",
    "                baseUrl += ('%2C' + colName)\n",
    "            else:\n",
    "                baseUrl += colName\n",
    "                bValid = True            \n",
    "    if not bValid:\n",
    "        return False\n",
    "    \n",
    "    bValid = False\n",
    "    for res in resolutions:\n",
    "        if res in resList:\n",
    "            baseUrl += ('&resolution=' + res)\n",
    "            bValid = True\n",
    "    if not bValid:\n",
    "        return False\n",
    "    baseUrl += '&ctype=csv'\n",
    "    \n",
    "    bResult = False\n",
    "    fileStarted = False\n",
    "    retryCount = 5\n",
    "    retryIndex = 0\n",
    "    step = 1000\n",
    "    offset = 0\n",
    "    downloadedCount = -1\n",
    "    dataAll = pd.DataFrame()\n",
    "    sUrls = []\n",
    "    if len(components) > 0:\n",
    "        for sProduct, vComponents in components.items():\n",
    "            for sComponent in vComponents:\n",
    "                sUrls.append(baseUrl + '&product=' + sProduct + '&component=' + sComponent)\n",
    "    else:\n",
    "        sUrls = [baseUrl]\n",
    "    print(len(components), len(sUrls))\n",
    "    print(sUrls)\n",
    "    for sUrl in sUrls:\n",
    "        print('start loading for: ', sUrl)\n",
    "        offset = 0\n",
    "        downloadedCount = -1\n",
    "        retryIndex = 0\n",
    "        while (offset < nMax) and (downloadedCount != 0):\n",
    "            try:\n",
    "                sCurrentUrl = sUrl + '&limit=' + str(min(step, nMax - offset)) + '&offset=' + str(offset)\n",
    "                print('Start downloading data', dt.datetime.now())\n",
    "                r = requests.get(sCurrentUrl, allow_redirects=True)\n",
    "               # with open(os.path.join(sDir, sName + '_Part.csv'), 'wb') as f:\n",
    "               #     f.write(r.content)\n",
    "               # dataPart = pd.read_csv(os.path.join(sDir, sName + '_Part.csv'), low_memory=False)\n",
    "                dataPart = pd.read_csv(io.BytesIO(r.content), low_memory=False)\n",
    "                dataPart['description'] = ''\n",
    "                dataPart.set_index(keys = 'bug_id', drop = False, inplace = True)\n",
    "                print('Start downloading extended data', dt.datetime.now())\n",
    "                downloadExtendedData(dataPart)\n",
    "                print('Finish downloading extended data', dt.datetime.now())\n",
    "                if dataAll.empty:\n",
    "                    dataAll = dataPart\n",
    "                else:\n",
    "                    dataAll = pd.concat([dataAll, dataPart], ignore_index=True)\n",
    "                downloadedCount = dataPart.shape[0]\n",
    "                offset += downloadedCount\n",
    "                print('downloaded at step: ' + str(downloadedCount))\n",
    "                print('downloaded all: ' + str(offset))\n",
    "                dataPart = pd.DataFrame()\n",
    "                retryIndex = 0\n",
    "                print('Finish processing data', dt.datetime.now())\n",
    "                gc.collect()\n",
    "            except Exception as exc:\n",
    "                print('error occured: ', exc)\n",
    "                if offset > 0:\n",
    "                    if fileStarted:\n",
    "                        with open(os.path.join(sDir, sName), 'a', encoding='utf-8') as f:\n",
    "                            dataAll.to_csv(f, header=False, encoding = 'utf-8')\n",
    "                            print('append data shape = ', dataAll.shape)\n",
    "                            dataAll = pd.DataFrame()\n",
    "                    else:\n",
    "                        dataAll.to_csv(os.path.join(sDir, sName), encoding = 'utf-8')\n",
    "                        print('write data shape = ', dataAll.shape)\n",
    "                        dataAll = pd.DataFrame()\n",
    "                        fileStarted = True\n",
    "                gc.collect()\n",
    "                if downloadedCount < step:\n",
    "                    retryIndex += 1\n",
    "                    time.sleep(61)\n",
    "                if retryIndex >= retryCount:\n",
    "                    print('reached max retries count: ', retryCount)\n",
    "                    break\n",
    "        if offset > 0:\n",
    "            if fileStarted:\n",
    "                with open(os.path.join(sDir, sName), 'a', encoding='utf-8') as f:\n",
    "                    dataAll.to_csv(f, header=False, encoding = 'utf-8')\n",
    "                    print('append data shape = ', dataAll.shape)\n",
    "                    dataAll = pd.DataFrame()\n",
    "            else:\n",
    "                dataAll.to_csv(os.path.join(sDir, sName), encoding = 'utf-8')\n",
    "                print('write data shape = ', dataAll.shape)\n",
    "                dataAll = pd.DataFrame()\n",
    "                fileStarted = True\n",
    "            gc.collect()\n",
    "            print('data loaded for: ', sUrl)\n",
    "            bResult = True\n",
    "    return bResult\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next cell provides basic constants and definition of many functions for work with datasets and models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 4000\n",
    "MAX_NB_WORDS = 5000\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "VALIDATION_SPLIT = 0.3\n",
    "MIN_PRODUCT_DESCRIPTIONS = 50\n",
    "MAX_PRODUCT_DESCRIPTIONS = 500000\n",
    "MIN_COMPONENT_DESCRIPTIONS = 50\n",
    "MAX_COMPONENT_DESCRIPTIONS = 500000\n",
    "CLASS_PRODUCT_NAME = 'product'\n",
    "CLASS_COMPONENT_NAME = 'component'\n",
    "\n",
    "BaseComponents = {\n",
    "                'Firefox': ['Untriaged', 'Developer Tools: Debugger', 'Session Restore', 'Developer Tools'],\n",
    "                'Core': ['DOM', 'General', 'XPCOM', 'Widget: Gtk'],\n",
    "                'Firefox for iOS': ['General'],\n",
    "                'NSS': ['Libraries']\n",
    "                 }\n",
    "\n",
    "BugsWaterPhrases = [' So, it is a bug. ', ' Fix it! ', ' Something wrong. ', ' Found error. ', ' There is a problem. ']\n",
    "\n",
    "def getKey(index, someDict):\n",
    "    return [key for key, value in someDict.items() if value == index][0]\n",
    "\n",
    "def clean_str2(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()    \n",
    "\n",
    "def engorgio(datalist, nCount = 1000):\n",
    "    n = len(datalist)\n",
    "    if (n == 0) or (n <= nCount // (2**len(BugsWaterPhrases))):\n",
    "        raise AssertionError('Cannot use engorgio for very small dataset with n= ' + str(n) + '; nCount = ' + str(nCount))\n",
    "    k = nCount // n + 1\n",
    "    extList = []\n",
    "    for item in datalist:\n",
    "        for i in range(2**k.bit_length()):\n",
    "            #print('test1')\n",
    "            baseItem = item\n",
    "            bsItem = format(i, 'b')\n",
    "            for j in range(len(bsItem)):\n",
    "                if bsItem[j] == '1':\n",
    "                    baseItem += BugsWaterPhrases[j]\n",
    "            extList.append(baseItem)\n",
    "        random.shuffle(extList)\n",
    "    return random.sample(extList, nCount)\n",
    "\n",
    "def reducio(datalist, nCount = 1000):\n",
    "    return random.sample(datalist, nCount)\n",
    "\n",
    "def balanse(datalist, nCount = 1000):\n",
    "#    if len(datalist) > nCount:\n",
    "#        return reducio(datalist, nCount)\n",
    "#    else:\n",
    "#        if len(datalist) < nCount:\n",
    "#            return engorgio(datalist, nCount)\n",
    "    return datalist\n",
    "\n",
    "def from_categorical(y_labels):\n",
    "    return np.argmax(y_labels, 1)\n",
    "\n",
    "def dict_len(d1):\n",
    "    nCount = 0\n",
    "    for key, value in d1.items():\n",
    "        nCount += len(value)\n",
    "    return nCount\n",
    "\n",
    "def dict_compare(d1, d2):\n",
    "    d1_keys = set(d1.keys())\n",
    "    d2_keys = set(d2.keys())\n",
    "    intersect_keys = d1_keys.intersection(d2_keys)\n",
    "    added = d1_keys - d2_keys\n",
    "    removed = d2_keys - d1_keys\n",
    "    modified = {o : (d1[o], d2[o]) for o in intersect_keys if d1[o] != d2[o]}\n",
    "    same = set(o for o in intersect_keys if d1[o] == d2[o])\n",
    "    return added, removed, modified, same\n",
    "\n",
    "def dict_save(d1, dictName = 'dict_temp_save', sDir = DATA_DIR_C ):\n",
    "    np.save(os.path.join(sDir, dictName + '.npy'), d1)    \n",
    "\n",
    "def dict_load(dictName = 'dict_temp_save', sDir = DATA_DIR_C ):\n",
    "    return np.load(os.path.join(sDir, dictName + '.npy')).item()\n",
    "\n",
    "def dataset_save(x_train, y_train, x_val, y_val, x_test, y_test,\n",
    "                 datasetName = 'current_dataset', sDir = DATA_DIR_C ):\n",
    "    np.savez(os.path.join(sDir, datasetName + '.npz'),\n",
    "             x_train = x_train,\n",
    "             y_train = y_train,\n",
    "             x_val = x_val,\n",
    "             y_val = y_val,\n",
    "             x_test = x_test,\n",
    "             y_test = y_test)\n",
    "\n",
    "# Warning: Unlike dataset_save, this function is more generalized, based on makeDataset.\n",
    "# Returns lists with different structure, based on splitCount.\n",
    "def dataset_load(datasetName = 'current_dataset', sDir = DATA_DIR_C, splitCount = 1 ):\n",
    "    if splitCount > 0:\n",
    "        if splitCount == 1:\n",
    "            dataset = np.load(os.path.join(sDir, datasetName + '.npz'))\n",
    "            return list(map(lambda x: dataset[x], dataset.files))\n",
    "        else:\n",
    "            resv = []\n",
    "            for i in range(splitCount):\n",
    "                dataset = np.load(os.path.join(sDir, datasetName + '_' + str(splitCount) + '_' + str(i) + '.npz'))\n",
    "                resv.append(list(map(lambda x: dataset[x], dataset.files)))\n",
    "            return resv\n",
    "    else:\n",
    "        return [dict_load(dictName = datasetName + '_data', sDir = sDir), \n",
    "                dict_load(dictName = datasetName + '_labels', sDir = sDir)]\n",
    "\n",
    "def load_datasets(sDir = DATA_DIR_C):\n",
    "    datasetsFiles = [f for f in os.listdir(sDir) if os.path.isfile(os.path.join(sDir, f)) and f.endswith('.npz')]\n",
    "    datasets = {s[:-len('.npz')] : dataset_load(s[:-len('.npz')], sDir) for s in datasetsFiles}\n",
    "    return datasets \n",
    "\n",
    "\n",
    "def embedding_bin2txt(sName = 'GoogleNews-vectors-negative300.bin', sDir = DATA_DIR_C):\n",
    "    model = KeyedVectors.load_word2vec_format(os.path.join(sDir, sName), binary=True)\n",
    "    model.wv.save_word2vec_format('GoogleNews-vectors-negative300.txt')\n",
    "    del model\n",
    "\n",
    "def embedding_matrix_load(wordIndex, sName = 'glove.6B.' + str(EMBEDDING_DIM) + 'd.txt',\n",
    "                          sDir = DATA_DIR_C, binary = False):\n",
    "    embeddings_index = {}\n",
    "    word2vec = None\n",
    "    if binary:\n",
    "        word2vec = KeyedVectors.load_word2vec_format(os.path.join(sDir, sName), binary=True)   \n",
    "    else:\n",
    "        with open(os.path.join(sDir, sName), mode='r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "    embedding_matrix = np.random.random((len(wordIndex) + 1, EMBEDDING_DIM))\n",
    "    if binary:\n",
    "        for word, i in wordIndex.items():\n",
    "            try:\n",
    "                embedding_matrix[i, :] = word2vec[word]\n",
    "            except KeyError:\n",
    "                pass        \n",
    "    else:\n",
    "        for word, i in wordIndex.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    gc.collect()\n",
    "    return embedding_matrix\n",
    "    \n",
    "def model_save(model, modelName = 'model_temp', sDir = MODELS_DIR_C):    \n",
    "    if modelName == '':\n",
    "        modelName = model.name\n",
    "    model.name = modelName\n",
    "    model_json = model.to_json()\n",
    "    with open(os.path.join(sDir, modelName + \".json\"), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(os.path.join(sDir, modelName + \".h5\"))\n",
    "    \n",
    "def model_load(modelName = 'model_temp', sDir = MODELS_DIR_C, bCompile = True):    \n",
    "    with open(os.path.join(sDir, modelName + '.json'), 'r') as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(os.path.join(sDir, modelName + \".h5\"))\n",
    "    if modelName != '':\n",
    "        loaded_model.name = modelName\n",
    "    if bCompile:\n",
    "        loaded_model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "    return loaded_model\n",
    "    \n",
    "def save_models(models, sDir = MODELS_DIR_C):\n",
    "    for name, model in models.items():\n",
    "        if model != None:\n",
    "            model_save(model, name, sDir)\n",
    "\n",
    "\n",
    "def load_models(sDir = MODELS_DIR_C, sStartsWith = ''):\n",
    "    modelsFiles = []\n",
    "    if sStartsWith == '':\n",
    "        modelsFiles = [f for f in os.listdir(sDir) if os.path.isfile(os.path.join(sDir, f)) and f.endswith('.json')]\n",
    "    else:\n",
    "        modelsFiles = [f for f in os.listdir(sDir) if (os.path.isfile(os.path.join(sDir, f)) and f.endswith('.json')\n",
    "                                                       and f.startswith(sStartsWith))]\n",
    "    models = {s[:-len('.json')] : model_load(s[:-len('.json')], sDir) for s in modelsFiles}\n",
    "    modelsScripts = []\n",
    "    if sStartsWith == '':\n",
    "        modelsScripts = [f for f in os.listdir(sDir) if os.path.isfile(os.path.join(sDir, f)) and f.endswith('.py')]\n",
    "    else:\n",
    "        modelsScripts = [f for f in os.listdir(sDir) if (os.path.isfile(os.path.join(sDir, f)) and f.endswith('.py')\n",
    "                                                         and f.startswith(sStartsWith))]\n",
    "    for s in modelsScripts:\n",
    "        models[s[:-len('.py')]] = None\n",
    "    return models\n",
    "\n",
    "def prepareData(sDir = DATA_DIR_C, sName = 'bugData.csv', maxClasses = 100, exactClasses = {},\n",
    "                minProductDescriptions = MIN_PRODUCT_DESCRIPTIONS, \n",
    "                maxProductDescriptions = MAX_PRODUCT_DESCRIPTIONS,\n",
    "                minComponentDescriptions = MIN_COMPONENT_DESCRIPTIONS,\n",
    "                maxComponentDescriptions = MAX_COMPONENT_DESCRIPTIONS,\n",
    "                otherPercents = 50,\n",
    "                balanseVirtual = True):\n",
    "    \n",
    "    if otherPercents < 0 or otherPercents >= 100:\n",
    "        raise AssertionError('invalid otherPercents value: ' + str(otherPercents))\n",
    "\n",
    "    product_data = {}\n",
    "    \n",
    "    data_frame = pd.read_csv(os.path.join(sDir, sName), low_memory=False)\n",
    "    print(data_frame.shape)\n",
    "    print(data_frame.columns)\n",
    "    data_frame.dropna(subset = ['description'], inplace = True)\n",
    "    print(data_frame.shape)\n",
    "    data_frame.drop([data_frame.columns[0]], axis = 1, inplace = True)\n",
    "    print(data_frame.shape)\n",
    "    print(data_frame.columns)\n",
    "    \n",
    "    productData = {}\n",
    "    productKeys = {}\n",
    "    \n",
    "    exactKeyNames = []\n",
    "    exactClasses\n",
    "    for key, vClasses in exactClasses.items():\n",
    "        for item in vClasses:\n",
    "            exactKeyNames.append(key + ' - ' + item)\n",
    "    \n",
    "    prod1 = data_frame[CLASS_PRODUCT_NAME].value_counts()\n",
    "    print(prod1.shape)\n",
    "    data_frame[data_frame[CLASS_PRODUCT_NAME].isin(prod1.where(prod1 < minProductDescriptions)\n",
    "                                          .dropna().index.tolist())] = 'Other ' + CLASS_PRODUCT_NAME + 's'\n",
    "    prod1 = data_frame[CLASS_PRODUCT_NAME].value_counts()\n",
    "    print(prod1)\n",
    "    prodNames = prod1.index.tolist()\n",
    "    for item in prod1.where(prod1 > maxProductDescriptions).dropna().index.tolist():\n",
    "        prodNames.remove(item)\n",
    "    n1 = len(prodNames)\n",
    "    \n",
    "    productData = { name: data_frame[data_frame[CLASS_PRODUCT_NAME] == name] for name in prodNames }\n",
    "\n",
    "    keyIndex = 0\n",
    "    product_data['Virtual Class'] = []\n",
    "    for name, prod_frame in productData.items():\n",
    "        comp1 = prod_frame[CLASS_COMPONENT_NAME].value_counts()\n",
    "        prod_frame[prod_frame[CLASS_COMPONENT_NAME].isin(comp1.where(comp1 < minComponentDescriptions)\n",
    "                                              .dropna().index.tolist())] = 'Other ' + CLASS_COMPONENT_NAME + 's'\n",
    "        comp1 = prod_frame[CLASS_COMPONENT_NAME].value_counts()\n",
    "        compNames = comp1.index.tolist()\n",
    "        for item in comp1.where(comp1 > maxComponentDescriptions).dropna().index.tolist():\n",
    "            compNames.remove(item)\n",
    "        n2 = len(compNames)\n",
    "        for key in compNames:\n",
    "            items = [' ||| '.join([x1, x2, x3, x4, x5]) for x1,x2,x3,x4,x5 in zip(\n",
    "                        balanse(prod_frame[prod_frame[CLASS_COMPONENT_NAME] == key]['reporter'].values.tolist()),\n",
    "                        balanse(prod_frame[prod_frame[CLASS_COMPONENT_NAME] == key]['short_desc'].values.tolist()),\n",
    "                        balanse(prod_frame[prod_frame[CLASS_COMPONENT_NAME] == key]['op_sys'].values.tolist()),\n",
    "                        balanse(prod_frame[prod_frame[CLASS_COMPONENT_NAME] == key]['rep_platform'].values.tolist()),\n",
    "                        balanse(list(map(lambda s: str(s), prod_frame[prod_frame[CLASS_COMPONENT_NAME] == key]['description']\n",
    "                                         .values.tolist()))))]\n",
    "            if len(exactKeyNames) > 0:\n",
    "                if name + ' - ' + key in exactKeyNames:                \n",
    "                    product_data[name + ' - ' + key] = items\n",
    "                else:\n",
    "                    if len(product_data['Virtual Class']) > 0:\n",
    "                        product_data['Virtual Class'].extend(items)\n",
    "                    else:\n",
    "                        product_data['Virtual Class'] = items\n",
    "            else:\n",
    "                if key != 'Other ' + CLASS_COMPONENT_NAME + 's':\n",
    "                    product_data[name + ' - ' + key] = items\n",
    "                else:\n",
    "                    if len(product_data['Virtual Class']) > 0:\n",
    "                        product_data['Virtual Class'].extend(items)\n",
    "                    else:\n",
    "                        product_data['Virtual Class'] = items\n",
    "                \n",
    "            productKeys[name + ' - ' + key] = keyIndex\n",
    "            keyIndex += 1\n",
    "            \n",
    "    if keyIndex <= maxClasses:\n",
    "        if balanseVirtual:\n",
    "            baseDataCount = 0\n",
    "            otherDataCount = 0\n",
    "            for key, texts in product_data.items():\n",
    "                if key == 'Virtual Class':\n",
    "                    otherDataCount += len(texts)\n",
    "                else:\n",
    "                    baseDataCount += len(texts)\n",
    "            print('baseDataCount = ', baseDataCount)\n",
    "            print('otherDataCount = ', otherDataCount)\n",
    "            # virtualCount / (virtualCount + baseDataCount) = otherPercents / 100\n",
    "            virtualCount = int(((baseDataCount * otherPercents) / 100.0) / (1.0 - otherPercents / 100.0))\n",
    "            print('virtualCount = ', virtualCount)\n",
    "            if otherDataCount > virtualCount:\n",
    "                print('balansing...')\n",
    "                product_data['Virtual Class'] = random.sample(product_data['Virtual Class'], virtualCount)\n",
    "    else:\n",
    "#        base_product_data = dict(sorted(product_data.items(), key=lambda k: len(k[1]), reverse=True)[:maxClasses])\n",
    "        other_product_data = dict(sorted(product_data.items(), key=lambda k: len(k[1]), reverse=True)[maxClasses+1:])\n",
    "        #list(map(product_data['Virtual Class'].extend, other_product_data))\n",
    "        for value in other_product_data.values():\n",
    "            product_data['Virtual Class'].extend(value)\n",
    "        product_data = dict(sorted(product_data.items(), key=lambda k: len(k[1]), reverse=True)[:maxClasses])\n",
    "        if balanseVirtual:\n",
    "            otherDataCount = len(product_data['Virtual Class'])\n",
    "            baseDataCount = dict_len(product_data) - otherDataCount\n",
    "            print('baseDataCount = ', baseDataCount)\n",
    "            print('otherDataCount = ', otherDataCount)\n",
    "            # virtualCount / (virtualCount + baseDataCount) = otherPercents / 100\n",
    "            virtualCount = int(((baseDataCount * otherPercents) / 100.0) / (1.0 - otherPercents / 100.0))\n",
    "            print('virtualCount = ', virtualCount)\n",
    "            if otherDataCount > virtualCount:\n",
    "                print('balansing...')\n",
    "                product_data['Virtual Class'] = random.sample(product_data['Virtual Class'], virtualCount)\n",
    "            \n",
    "        print({key: len(value) for key, value in product_data.items() })\n",
    "        productKeys = {list(product_data.keys())[i] : i for i in range(len(product_data))}\n",
    "        print(productKeys)\n",
    "\n",
    "    data_frame = pd.DataFrame()\n",
    "    gc.collect()\n",
    "    print('prepareData finished')\n",
    "    return product_data, productKeys\n",
    "\n",
    "def makeDataset(data_dict, productKeys, datasetName = 'current_dataset', sDir = DATA_DIR_C,\n",
    "                splitCount = 1, splitVirtualOnly = False, maxCount = 0):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    print('productKeys len = ', len(productKeys))\n",
    "    print('data_dict len = ', len(data_dict))\n",
    "    \n",
    "    if splitVirtualOnly == False:\n",
    "        for key, value in data_dict.items():\n",
    "            for sDesc in value:\n",
    "                text = BeautifulSoup(sDesc, \"lxml\")\n",
    "                sText = clean_str2(text.get_text())\n",
    "                if sText not in texts:\n",
    "                    texts.append(sText)\n",
    "                    labels.append(productKeys[key])\n",
    "            print('converting text for key: ' + str(key) + ' with len = ' + str(len(value)) + ' finished')\n",
    "\n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "        word_index = tokenizer.word_index\n",
    "        print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "        data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "        labels = to_categorical(np.asarray(labels))\n",
    "        print('Shape of data tensor:', data.shape)\n",
    "        print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "        indices = np.arange(data.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        if maxCount > 0 and indices.shape[0] > maxCount:\n",
    "            indices = random.sample(indices, maxCount)\n",
    "        data = data[indices]\n",
    "        labels = labels[indices]\n",
    "        nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "        if splitCount > 0:\n",
    "            x_train = data[:-nb_validation_samples]\n",
    "            y_train = labels[:-nb_validation_samples]\n",
    "            x_val = data[-nb_validation_samples: -(nb_validation_samples // 2)]\n",
    "            y_val = labels[-nb_validation_samples: -(nb_validation_samples // 2)]\n",
    "            x_test = data[-(nb_validation_samples // 2):]\n",
    "            y_test = labels[-(nb_validation_samples // 2):]\n",
    "            dataset_save(x_train, y_train, x_val, y_val, x_test, y_test,\n",
    "                         datasetName=datasetName, sDir = sDir)\n",
    "            if splitCount > 1:\n",
    "                def makePart(X, n, i):\n",
    "                    return X[(i*len(X))//n : ((i+1)*len(X))//n]\n",
    "                for i in range(splitCount):\n",
    "                    dataset_save(makePart(x_train, splitCount, i),\n",
    "                                 makePart(y_train, splitCount, i),\n",
    "                                 makePart(x_val, splitCount, i),\n",
    "                                 makePart(y_val, splitCount, i),\n",
    "                                 makePart(x_test, splitCount, i),\n",
    "                                 makePart(y_test, splitCount, i),\n",
    "                                 datasetName=datasetName + '_' + str(splitCount) + '_' + str(i),\n",
    "                                 sDir = sDir)\n",
    "        else:\n",
    "            dict_save(data, datasetName + '_data', sDir = sDir)\n",
    "            dict_save(labels, datasetName + '_labels', sDir = sDir)\n",
    "\n",
    "        dict_save(word_index, datasetName + '_word_index', sDir = sDir)\n",
    "    else:\n",
    "        raise NotImplementedError('''May be used in future to create ensembles with model, \n",
    "                                     trained on few datasets with equal base classes,\n",
    "                                     but different parts of virtual classes data''')\n",
    "    \n",
    "    print('makeDataset finished')\n",
    "#    print([np.array_equal(x, y) for x,y in zip([x_train, y_train, x_val, y_val, x_test, y_test], dataset_load())])\n",
    "    \n",
    "\n",
    "\n",
    "def testModel(resultsFrame, modelName = 'current_model', datasetName = 'current_dataset',\n",
    "              dataDir = DATA_DIR_C, \n",
    "              modelsDir = MODELS_DIR_C):\n",
    "    bModelReady = (os.path.exists(os.path.join(modelsDir, modelName + '.json')) and \n",
    "                   os.path.exists(os.path.join(modelsDir, modelName + '.h5')))\n",
    "    if bModelReady:\n",
    "        model = model_load(modelName=modelName, sDir=modelsDir)\n",
    "        x_train, y_train, x_val, y_val, x_test, y_test = dataset_load(datasetName=datasetName, sDir=dataDir)\n",
    "        try:\n",
    "            scores = model.evaluate(x_test, y_test)\n",
    "            resultsFrame.set_value(modelName, datasetName, scores[1])\n",
    "        except Exception as exc:\n",
    "            print(exc)\n",
    "    else:\n",
    "        try:\n",
    "            mfile = importlib.import_module(name = modelName)\n",
    "            importlib.reload(mfile)\n",
    "            try:\n",
    "                mfile.set_params(params)\n",
    "            except Exception as exc2:\n",
    "                print(exc2)\n",
    "            resultsFrame.set_value(modelName, datasetName, mfile.test_model(modelName, datasetName, x_test, y_test))\n",
    "        except Exception as exc:\n",
    "            print(exc)\n",
    "    \n",
    "       \n",
    "def testModels(dataDir = DATA_DIR_C, modelsDir = MODELS_DIR_C):\n",
    "    datasets = load_datasets(dataDir)\n",
    "    models = load_models(modelsDir)\n",
    "    \n",
    "    resultsFrame = pd.DataFrame(columns = [datasetName for datasetName, _ in datasets.items()])\n",
    "\n",
    "    for datasetName, [x_train, y_train, x_val, y_val, x_test, y_test] in datasets.items():\n",
    "        for modelName, model in models.items():\n",
    "            try:\n",
    "                if model != None:\n",
    "                    scores = model.evaluate(x_test, y_test)\n",
    "                    resultsFrame.set_value(modelName, datasetName, scores[1])\n",
    "                else:\n",
    "                    if os.path.exists(os.path.join(modelsDir, modelName + '_' + datasetName + '.json')):\n",
    "                        model = load_model(modelName + '_' + datasetName, modelsDir)\n",
    "                        scores = model.evaluate(x_test, y_test)\n",
    "                        resultsFrame.set_value(modelName, datasetName, scores[1])\n",
    "                    else:\n",
    "                        if os.path.exists(modelsDir) and (modelsDir not in sys.path):\n",
    "                            sys.path.append(modelsDir)\n",
    "                        mfile = importlib.import_module(name = modelName)\n",
    "                        importlib.reload(mfile)\n",
    "                        resultsFrame.set_value(modelName, datasetName, mfile.test_model(modelName, datasetName, \n",
    "                                                                                        x_test, y_test))\n",
    "            except Exception as exc:\n",
    "                print(exc)\n",
    "            #gc.collect()\n",
    "    \n",
    "    resultsFrame.to_csv(os.path.join(dataDir, 'current_results.csv'))\n",
    "    print()\n",
    "    print(resultsFrame)\n",
    "    \n",
    "    print('testModels finished')\n",
    "    \n",
    "def reloadSession():\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    cfg = K.tf.ConfigProto()\n",
    "    cfg.gpu_options.allow_growth = True\n",
    "    K.set_session(K.tf.Session(config=cfg))\n",
    "\n",
    "def trainModel(modelName = 'current_model', datasetName = 'current_dataset', \n",
    "               dataDir = DATA_DIR_C,\n",
    "               modelsDir = MODELS_DIR_C):\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = dataset_load(datasetName, sDir=dataDir)\n",
    "    word_index = dict_load(datasetName + '_word_index', sDir=dataDir)\n",
    "    \n",
    "    embedding_matrix = embedding_matrix_load(word_index, sDir=dataDir) \n",
    "#                                             sName='GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    \n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    \n",
    "#'model': 'model_opt_py_dataset_exact10_4000_100d_64_0.1_3_256_15_nadam_local_same_LeakyRelu_0.2', \n",
    "#'params': ['64', '0.1', '3', '256', '15', 'nadam', 'local', 'same', 'LeakyRelu', '0.2'], \n",
    "#'results': [0.554242658161845, 0.8583016476854777]\n",
    "\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    l_sdrop1 = SpatialDropout1D(0.15)(embedded_sequences)\n",
    "    l_conv1 = Conv1D(256, 3, padding='same')(l_sdrop1)\n",
    "    l_act1 = LeakyReLU(0.2)(l_conv1)\n",
    "    l_sdrop2 = SpatialDropout1D(0.15)(l_act1)\n",
    "    l_pool1 = MaxPooling1D(MAX_SEQUENCE_LENGTH )(l_sdrop2)\n",
    "    l_flat = Flatten()(l_pool1)\n",
    "    preds = Dense(y_val.shape[1], activation='softmax')(l_flat)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='nadam',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    model.summary()\n",
    "    try:\n",
    "        model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "                  epochs=5, batch_size=32,\n",
    "                  callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=2, verbose=1, \n",
    "                                                             mode='auto')])\n",
    "        print(model.evaluate(x_test, y_test))\n",
    "        model_save(model, modelName, sDir=modelsDir)\n",
    "    except Exception as exc:\n",
    "        print(exc)\n",
    "        model_save(model, modelName + '_temp', sDir=modelsDir)    \n",
    "    print('trainModel finished')\n",
    "    del model\n",
    "    reloadSession()\n",
    "  \n",
    "    \n",
    "    \n",
    "print('ready')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next cell for downloading bugs data from bugzilla.\n",
    "Since we provided the data along with the script, you can skip this step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloadData(resolutions = ['FIXED', 'WONTFIX'], sName = 'bugDataExactComponents100_3.csv', \n",
    "#             nMax = 100000, components=BaseComponents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next cell defines current dataset and model names.\n",
    "May be used to create more datasets with different parameters and for in-place training and testing models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CurrentDatasetName = 'dataset_global50_exv4' + '_' + str(MAX_SEQUENCE_LENGTH) + '_' + str(EMBEDDING_DIM) + 'd'\n",
    "CurrentModelName = 'cnn50GlobalExv4' + '_' + str(MAX_SEQUENCE_LENGTH) + '_' + str(EMBEDDING_DIM) + 'd'\n",
    "\n",
    "# maxClasses = 11 (10 + virtual class with other components)\n",
    "product_data, product_keys = prepareData(sName='bugDataTest500.csv', maxClasses=51, \n",
    "                                         minComponentDescriptions=50,\n",
    "                                         minProductDescriptions=50)\n",
    "\n",
    "makeDataset(datasetName=CurrentDatasetName,\n",
    "            data_dict=product_data, productKeys=product_keys)\n",
    "\n",
    "#reloadSession()\n",
    "trainModel(datasetName=CurrentDatasetName, modelName=CurrentModelName)\n",
    "\n",
    "#testModels()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp parts\n",
    "\n",
    "#resListAll = [\"FIXED\", \"INVALID\", \"WONTFIX\", \"DUPLICATE\", \"WORKSFORME\", \"INCOMPLETE\",\n",
    "#               \"SUPPORT\", \"EXPIRED\", \"MOVED\"]\n",
    "#downloadData(resolutions = ['FIXED', 'WONTFIX'], sName = 'bugDataTest500_2.csv', nMax = 500000)\n",
    "\n",
    "#resultsFrame = pd.DataFrame(columns = ['dataset_exact10'])\n",
    "#testModel(resultsFrame=resultsFrame, datasetName='dataset_exact10', modelName='model_opt_py')\n",
    "#print()\n",
    "#print(resultsFrame)\n",
    "#makeDataset(datasetName='ExampleSet', data_dict=product_data, productKeys=product_keys)\n",
    "#trainModel(datasetName='ExampleSet', modelName='ExampleModel')\n",
    "\n",
    "#product_data, product_keys = prepareData(sName='bugDataTest500.csv', exactClasses=BaseComponents)\n",
    "\n",
    "#product_data, product_keys = prepareData(sName='bugDataExactComponents100_2.csv', exactClasses=BaseComponents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next cell provide functionality of automatically training and saving models, selecting best model from defined parameters.\n",
    "This cell can interact with any model, defined in python file which has next methods:\n",
    "\n",
    "def set_params(params):\n",
    "\n",
    "def test_model(modelName, datasetName, px_test, py_test, sequenceInput = None):\n",
    "\n",
    "test_model function must return float value score.\n",
    "\n",
    "There are a few models, provided with this notebook:\n",
    "\n",
    "model_opt_py (CNN), model_rnn_py (RNN), model_rcnn_py (CNN + LSTM).\n",
    "\n",
    "They can be used in selectModel function in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def getModelParamData(s):\n",
    "    kStart = s.find('(')\n",
    "    kEnd = s.rfind(')')\n",
    "    res = []\n",
    "    if kStart != -1 and kEnd != -1 and (kEnd > kStart + 1):\n",
    "        res.append(s[:kStart])\n",
    "        res += s[kStart + 1: kEnd].split(',')\n",
    "    else:\n",
    "        res.append(s)\n",
    "    return res\n",
    "'''\n",
    "\n",
    "#CurrentDatasetName = 'dataset_exact10' + '_' + str(MAX_SEQUENCE_LENGTH) + '_' + str(EMBEDDING_DIM) + 'd'\n",
    "#CurrentModelName = 'cnnOptExact10Local' + '_' + str(MAX_SEQUENCE_LENGTH) + '_' + str(EMBEDDING_DIM) + 'd'\n",
    "\n",
    "#reloadSession()\n",
    "ParamsVDict = {'batch_size': [32],\n",
    "               'dropout': [0.15, 0.2],\n",
    "               'conv_size': [3],\n",
    "               'epochs': [6],\n",
    "               'optimizer': ['nadam'], #rmsprop\n",
    "               'conv_filters': [256],               \n",
    "               'pooling': ['global'], #local\n",
    "               'padding': ['same'], #valid\n",
    "               'activation': ['relu', 'LeakyRelu(0.1)', 'LeakyRelu(0.2)'],\n",
    "               'lstm_units': [80],\n",
    "               'use_pretrained': [1], # use pretrained glove vector (1) or train from zero (0).\n",
    "               'name_suffix': ['suffixCnne1'] # just a name suffix. May be used to train same models few times.\n",
    "              }\n",
    "\n",
    "def getParams(sName, index = 0, ModelParams = ParamsVDict):\n",
    "    return ModelParams[sName][index].split('_')\n",
    "\n",
    "# In this function, you can select name of python file with model, which must contains a few functions\n",
    "# def set_params(params):\n",
    "# def test_model(modelName, datasetName, px_test, py_test, sequenceInput = None): # must return float score between 0 and 1.\n",
    "# function will find best parameters for model to get best score.\n",
    "def selectModel(modelName = 'model_test_py', datasetName = 'dataset_exact10', modelsDir = MODELS_DIR_C):\n",
    "\n",
    "    # lengths of param vectors\n",
    "    # 2 3 3 2\n",
    "    ldata = [len(v) for name, v in ParamsVDict.items()]\n",
    "    nMax = reduce(lambda x, y: x * y, ldata)\n",
    "    # products of lengths of param vectors in reverse order except first number.\n",
    "    # 3*3*2 3*2 2\n",
    "    ldata2 = [reduce(lambda x, y: x * y, ldata[len(ldata):n1:-1], 1) for n1 in range(0, len(ldata) - 1)]\n",
    "    \n",
    " #   print('ldata: ', ldata)\n",
    " #   print('nMax = ', nMax)\n",
    " #   print('ldata2: ',ldata2)\n",
    "    \n",
    "    # transform index to sequence of params from ParamsVDict\n",
    "    # this process is similar to transforming value to other number system (like factorial number system)\n",
    "    def getNextParams(index, paramsDict = ParamsVDict):\n",
    "   #     ldata = [len(v) for name, v in paramsDict.items()]\n",
    "   #     nMax = reduce(lambda x, y: x * y, ldata)\n",
    "   #     ldata2 = [reduce(lambda x, y: x * y, ldata[len(ldata):n1:-1], 1) for n1 in range(0, len(ldata) - 1)]\n",
    "        if index >= nMax or index < 0:\n",
    "            print('getNextParams: index is out of range ', index)\n",
    "            return {}\n",
    "        res = {}\n",
    "        itemIndex = 0\n",
    "        num = index\n",
    "        print('num = ', num)\n",
    "        for item, v in paramsDict.items():\n",
    "            if itemIndex < len(ldata2):\n",
    "                k = num // ldata2[itemIndex]\n",
    "                print(itemIndex, k)\n",
    "                res[item] = v[k]\n",
    "                num -= k * ldata2[itemIndex]\n",
    "            else:\n",
    "                res[item] = v[num]\n",
    "            itemIndex += 1\n",
    "        print('res = ',res)\n",
    "        return res\n",
    "    \n",
    "    _,_,_,_, x_test, y_test = dataset_load(datasetName)\n",
    "\n",
    "    if modelsDir not in sys.path:\n",
    "        sys.path.append(modelsDir)\n",
    "    mfile = importlib.import_module(modelName)\n",
    "\n",
    "    bestScore = 0\n",
    "    bestParams = {}\n",
    "    index = 0\n",
    "    while index < nMax:\n",
    "        currentParams = getNextParams(index)\n",
    "        if (len(currentParams) > 0):\n",
    "            print(currentParams)\n",
    "            importlib.reload(mfile)\n",
    "            try:\n",
    "                mfile.set_params(currentParams)\n",
    "            except Exception as exc:\n",
    "                print('unsupported setting parameters. Finish after first iteration.')\n",
    "                if index > 0:\n",
    "                    break\n",
    "            curScore = mfile.test_model(modelName, datasetName, x_test, y_test)\n",
    "            if curScore > bestScore:\n",
    "                bestScore = curScore\n",
    "                bestParams = currentParams\n",
    "                print(bestScore)\n",
    "            gc.collect()\n",
    "        index += 1\n",
    "        if len(currentParams) <= 0:\n",
    "            break\n",
    "            \n",
    "    print(bestScore)\n",
    "    print(bestParams)\n",
    "    \n",
    "selectModel(modelName = 'model_opt_py', datasetName=CurrentDatasetName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The previous cell provides possibility to select best parameters for almost any model. \n",
    "However, the next cell provides ability to collect and save selected models information, but only for Keras models, assuming, that models parameters contained in models names, divided by symbol '_'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def makeModelName(params, modelName = 'model_opt_py', datasetName = CurrentDatasetName):\n",
    "    sfName = modelName + '_' + datasetName\n",
    "    for key, value in params.items():\n",
    "        sfName += ('_' + str(value))\n",
    "    return sfName\n",
    "\n",
    "def calc_models_variants(x_test, y_test, sDir = MODELS_DIR_C, baseModelNames = ['model_opt_py'],\n",
    "                         params=ParamsVDict, sNamePart = ''):\n",
    "    vmodels = []\n",
    "    if len(baseModelNames) == 0:\n",
    "        print('Warning: calc_model_variants: empty base models list')\n",
    "        return []\n",
    "    for baseModelName in baseModelNames:\n",
    "        modelsFiles = []\n",
    "        if baseModelName == '':\n",
    "            continue\n",
    "#            modelsFiles = [f for f in os.listdir(sDir) if os.path.isfile(os.path.join(sDir, f)) and f.endswith('.json')]\n",
    "        else:\n",
    "            modelsFiles = [f for f in os.listdir(sDir) if (os.path.isfile(os.path.join(sDir, f))\n",
    "                                                           and f.endswith('.json')\n",
    "                                                           and f.startswith(baseModelName)\n",
    "                                                           and (True if sNamePart == '' else sNamePart in f))]\n",
    "        index = 0\n",
    "        # models element description: \n",
    "        # {'model' : loaded keras model, 'params' : [params list], 'results': [scores list] }\n",
    "        shift1 = 0\n",
    "        for s in modelsFiles:\n",
    "            item = {}\n",
    "            model = model_load(s[:-len('.json')], sDir, bCompile=False)\n",
    "    #        model.name = s[:-len('.json')]\n",
    "            item['path'] = sDir\n",
    "            item['model'] = s[:-len('.json')]\n",
    "            if s[len(baseModelName)] == '_':\n",
    "                shift1 = 1\n",
    "            else:\n",
    "                shift1 = 0\n",
    "            item['params'] = s[len(baseModelName) + shift1 : -len('.json')].split('_')\n",
    "            sOptimizer = 'nadam'\n",
    "            if 'rmsprop' in item['params']:\n",
    "                sOptimizer = 'rmsprop'\n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=sOptimizer,\n",
    "                          metrics=['acc'])\n",
    "            try:\n",
    "                item['results'] = model.evaluate(x_test, y_test)\n",
    "                vmodels.append(item)\n",
    "            except Exception as exc:\n",
    "                print('Cannot evaluate model with current test set: ' + s[:-len('.json')])\n",
    "                print(exc)\n",
    "            del model\n",
    "            index += 1\n",
    "            if (index % 10 == 0):\n",
    "                reloadSession()\n",
    "        reloadSession()\n",
    "    return vmodels\n",
    "    \n",
    "def writeModelsData(modelsData, fData = 'modelsInfo.txt', sDir = LOGS_DIR_C, bHistory = True):\n",
    "    if bHistory:\n",
    "        with open(os.path.join(sDir, fData), 'at') as f:\n",
    "            for modelData in modelsData:\n",
    "                json.dump(modelData, f, indent=4)\n",
    "                print(file=f)\n",
    "    else:\n",
    "        with open(os.path.join(sDir, fData), 'wt') as f:\n",
    "            json.dump(modelsData, f)\n",
    "            \n",
    "def loadModelsData(fData = 'modelsData.json', sDir = DATA_DIR_C):\n",
    "    with open(os.path.join(sDir, fData), 'rt') as f:\n",
    "        return json.load(f)\n",
    "            \n",
    "\n",
    "_,_,_,_, x_test, y_test = dataset_load(CurrentDatasetName)\n",
    "\n",
    "reloadSession()\n",
    "\n",
    "historyFile = 'modelsInfo.txt'\n",
    "\n",
    "#BaseModelNames = ['model_rcnn_py', 'model_opt_py', 'model_rnn_py']\n",
    "BaseModelNames = ['model_opt_py']\n",
    "\n",
    "def printBaseModelInfo():\n",
    "    with open(os.path.join(LOGS_DIR_C, historyFile), 'at') as f:\n",
    "        print(file=f)\n",
    "        print('Models based on: ', file=f)\n",
    "        print(BaseModelNames, file=f)\n",
    "        print('Used dataset: ' + CurrentDatasetName, file=f)\n",
    "\n",
    "# Disable it for temp test models or models, which has been already documented\n",
    "printBaseModelInfo()       \n",
    "\n",
    "modelsData = calc_models_variants(x_test, y_test, baseModelNames=[baseModelName + '_' + CurrentDatasetName + '_' \n",
    "                                                  for baseModelName in BaseModelNames])\n",
    "writeModelsData(modelsData, historyFile, LOGS_DIR_C)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
